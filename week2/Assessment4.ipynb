{"cells":[{"cell_type":"markdown","source":["## Open our dataset\n","Lets start by opening our csf file and have a look at it"],"metadata":{"id":"PYLAgR4QmyGz"}},{"cell_type":"code","execution_count":null,"source":["import pandas as pd \n","df = pd.read_csv(\"data/gradd.csv\") \n","df.head()"],"outputs":[],"metadata":{"id":"vv9lFT8wmyG7"}},{"cell_type":"markdown","source":["Now copy out our X and y columns into series"],"metadata":{"id":"Crk86E9kmyG-"}},{"cell_type":"code","execution_count":null,"source":["import numpy as np\n","X = np.matrix(df[\"x\"].to_numpy()).transpose() \n","y = np.matrix(df[\"y\"].to_numpy()).transpose() \n","\n","m,n = X.shape"],"outputs":[],"metadata":{"id":"B4rjvGWHmyG_"}},{"cell_type":"markdown","source":["## Normalise X\n","Now, lets normalise X so the values lie between -1 and 1. We do this so we can get all features into a similar range. We use the following equation  \n","$X_{(i)} = \\frac{x_{(i)}-mean(x)}{max(x)-min(x)}$"],"metadata":{"id":"ttnCOkabmyG_"}},{"cell_type":"code","execution_count":null,"source":["mu = X.mean() # \n","sigma = X.std() # standard deviation: max(x)-min(x)\n","xn = (X - mu) / sigma\n","#xn2 = (X - X.mean()) / (X.max() - X.min())"],"outputs":[],"metadata":{"id":"FzvL6vPAmyHA"}},{"cell_type":"markdown","source":["Add a column of ones to X for easier matric manipulation of our hypothesis and cost function later"],"metadata":{"id":"AJ8yYzOfmyHB"}},{"cell_type":"code","execution_count":null,"source":["\n","xo = np.matrix(np.hstack((np.ones((m,1)),xn)))"],"outputs":[],"metadata":{"id":"IhSGqyTPmyHD"}},{"cell_type":"markdown","source":["## Gradient descent\n","\n","Create the variables we need for gradient descent"],"metadata":{"id":"joqUfOc_myHE"}},{"cell_type":"code","execution_count":null,"source":["repeat = 100\n","lrate = 0.5\n","theta = np.matrix([0,0]).transpose()\n","costhistory = np.zeros((repeat,1))"],"outputs":[],"metadata":{"id":"gq8SF5mZmyHF"}},{"cell_type":"markdown","source":["\n","We now go into our gradient descent loop, where we calculate a new theta on each loop and keep track of its cost.  \n","Repeat untill convergance {  \n","&nbsp;&nbsp;&nbsp;&nbsp;`First calculate the hypothesis and then its cost with equation`  \n","&nbsp;&nbsp;&nbsp;&nbsp;$hc^{(i)} = h\\theta(x^{(i)}) -y^{(i)}$  \n","\n","&nbsp;&nbsp;&nbsp;&nbsp;`Calculate new thetas using the learning rate. This equation is made easier with matrix`  \n","&nbsp;&nbsp;&nbsp;&nbsp;`manipulation and the fact that we added a column of ones to X`  \n","&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_{n} = \\theta_{n} - \\alpha (\\frac{1}{m}) \\sum \\limits _{j=1}hc^{(i)}.x^{(i)}$\n","  \n","&nbsp;&nbsp;&nbsp;&nbsp;`Keep track of the cost of the new theta as we go:`  \n","&nbsp;&nbsp;&nbsp;&nbsp;$J^{(i)} = (\\frac{1}{2m}) \\sum \\limits _{j=1}(h\\theta(x^{(i)}) -y^{(i)})^2$  \n","}"],"metadata":{"id":"z9V-ZZommyHF"}},{"cell_type":"code","execution_count":null,"source":["#costhistory = pd.DataFrame([0] * repetition, columns=['cost'])\n","for i in range(repeat):\n","    #calculate cost of hypothesis\n","    hc = xo * theta - y\n","    temp = hc.transpose() * xo\n","    #new theta\n","    theta = theta - (lrate * (1/m)) * temp.transpose() \n","    #recalculate cost of hypothesis with new theta\n","    hc = xo * theta - y \n","    costhistory[i] = (hc.transpose() * hc) / (2*m)  "],"outputs":[],"metadata":{"id":"3xtjfgF7myHG"}},{"cell_type":"markdown","source":["## Plot the cost history\n","Plot the costs history from gradient descent to ensure its reducing over time. You can pick up from the graph how many repitions you need with your dataset and can adjust `repeat` variable above. You can also adjust `lrate` to "],"metadata":{"id":"q-JdXNMDmyHH"}},{"cell_type":"code","execution_count":null,"source":["import matplotlib.pyplot as plt\n","plt.plot(costhistory,list(range(0, repeat)))\n","plt.ylabel('cost')\n","plt.xlabel('repitition')\n","plt.show()"],"outputs":[],"metadata":{"id":"qmuksHmLmyHH"}},{"cell_type":"markdown","source":["## Plot the dataset and hypothesis"],"metadata":{"id":"QUkZCitzmyHI"}},{"cell_type":"code","execution_count":null,"source":["plt.plot(np.ravel(xn),np.ravel(y),'rx')\n","plt.plot([xn.min(),xn.max()], np.ravel([theta[0]+theta[1]*xn.min(),theta[0]+theta[1]*xn.max()]))"],"outputs":[],"metadata":{"id":"L18EvfrFmyHI"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"saEIpPDJmyHJ"}}],"metadata":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"name":"Assessment4.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}